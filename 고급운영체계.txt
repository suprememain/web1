1. Microsoft Win32 응용에서 사용할 수 있는 Thread 생성 함수 3가지의 원형과 입력/출력 파라미터를 설명하고 그 차이를 기술
-IA-32 Architecture
* The memory management of IA-32 incorporates segmentation as well as paging scheme

[CPU] -logical address-> [segmentation unit] -linear address-> [paging unit] -physical address-> [physical memory]

 . Segmentation Unit: 16-bits selector with 32- bits offset of logical address structure (48 bits in total)
 - selector comprises of 13-bit segment (8K segment items can be designated),
  1-bit table indicator (one for GDT - Global Descriptor Table - for shared segments, and the other one for LDT - Local Descriptor Table - for private segments of a process), and 2-bits protection
 - 16-bits selector can accomodate 16K number of segments in total (8K for global and 8K for local),
  each of which is of the size 4GB (32 bits offset)
 - based on the descriptor table indexed by the selector, 32-bits linear address is generated.




6. Windows XP OS의 스케쥴러 특징





7. Segmentation을 통한 OS의 메모리 관리를 설명하고, 이를 통해 얻을 수 있는 이점을 기술

* A logical address of a process is a collection of segments (sub-routines, functions, stack amounts, etc.) , which may have their own respective segment base (segment number) and offset (within limit register bound).

 . In case of C compiler, the generated segments are 1) the code, 2) global variables, 3) the heap, 4) the stacks of each thread, and 5) the standard libraries (i.e., different memory area - stack, data, heap, code)
 . The loader (or linker) would take all these segments and assign them segment numbers (which will be used by logical address)

* Address binding with segmentation is supported by hardware using the segment table,
 which is an array of base-limit register pairs for each segment.
 . By using separate base registers, segmentation can use non-contiguous memory for a process
 . The base-limit register usage may requires 'compaction' on the basis of segment (for the contiguous allocation for each segment)



8. Demand Paging 기법에서 page replacement 방법이 중요한 이유를 설명하고 Second Chance 알고리즘과 Enhanced Second Chance 알고리즘에 대해 기술

* Windows uses demand paging with clustering, meaning they page in multiple pages whenever a page fault occurs.
 . Working set minimum and maximum are normally set at 50 and 345 pages, respectively
 . If a page fault occurs and the process is below their maximum (~1.3MB), additional pages are allocated.
 Otherwise some pages from this process most be replaced, using a local replacement algorithm.

* If the amount of free frames falls below the allowable threshols, then working set trimming occurs,
taking frames away from any processes which are above their minimum, until all are at their minimums.
 Then additional frames can be allocated to processes that need them.

* Page replacement algorithm (selecting victims) depends on the type of processor
 . single processor x86 - a variation of second-chance algorithm is used
 . alpha or multiprocessor - a variation of FIFO is used
                                  (beacuse clearing reference its require invalidating entires in the TLB on other processors - expensive)


=====================================
- Basic Concept
* The idea of demand paging is that when a process is swapped in, the swapper (or page) loads only a portion of pages that is expected to be needed (right away)
 . Pages that are not loaded into memory are marked as invalid in the page table (using invalid bit)
 . If a page is needed that was not originally loaded up, then a page fault trap is generated, which is handled in septs of:
  1) requested memory address is checked whether it is valid or not
  2) when the reference is invalid, OS preenpt the system for handling the page fault trap
  3) OS checks available memory frames from a free-frame list (for allocating a page or swapping in a page)
  4) a disk operation is scheduled to bring in the necessary from disk (I/O blocking is occured for the current process)
  5) when the I/O completes, the process's page table is updated with the new frame number and the valid bit is set
  6) the instruction that caused the page fault is restarted from the beginning



* Pure Demand Paging - NO pages are swapped in for a process until they are requested by page faults (an extreme case)
 . In theory each instruction could generate multiple page faults, but this is very rare in practice because of the locality of reference
 . The hardware supporting virtual memory is the same as for paging and swapping


 - Performance of Demand Paging
* Page fault slowdown the system (swapping goes get data from disk), and thus its effect needs to be analyzed
 . suppose that a normal memory access requires 2ns (500MHz access) and serving page fault 80us (40,000 times of normal access)
  With a page fault rate of p, the effective access time will be

     E = (l-p) * (2) + p*(80000) = 2 + 79,998*p

 . With page fault rate of 0.001 (once at every 1,000 times memory access), the effective access time drops to 82ns (41 times)
 In order to keep the slowdowm factor less than 10%, the page fault rate must be less than 0.0000025 (once in 399.990 accesses)
 . Swap space is generally faster than the regular file system, because it does not have to go through the whole directory structure.
 For this reason, some systems transfer an entire process from the file system to swap space before starting up the process.
 . Some systems use demand paging directly from the file systme for program code, and to reserve the swap space for data segments.
 This is because the program code never changes and hence does not have to be stored on page operation (Solaris and BSD Unix)



9. TLB (Translation Lookaside Buffer)를 가지는 paging 시스템에서 Virtual Address를 사용하는 CPU가 특정 메모리 번지의 데이터를 접근하는 과정을 설명하세요 (단, 빠른 데이터 접근을 위해 해당 시스템은 data cache를 사용한다고 가정함)

=======================
Data Access Mechanism

- virtual address space is translated to physical address space to access data

* TLB is checked first to find whether the requested data is in the memory or not
 . 64 items (128 for instruction) for data TLB (level 1) with 512 shared entreis of Level 2 TLB in Intel I7
* note that TLB is shared by multiple cores
 . TLB miss requires two times of memory access for page translation via page table
* TLB/cache miss and corruption
 . internal fragmentation of cache lines and pages make data loaded in TLB/cache useless
 . cache alignment (starting data address coincide with cache line size) and data allocation within a page (or a number of pages) need to be controlled in application program

